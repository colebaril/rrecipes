# Load Packages 
library(tidyverse)
library(rvest)
library(stringi)

# Allrecipes (https://www.allrecipes.com/)

# Function for scraping HTML
# The `write.table()` function is set to append each recipe after the other each time a new URL passes through the function. 
# Beware that this will overwrite the file every time if the file is not changed. 
# The 1/3 and 1/8 Unicode characters are bugged and do not export correctly in R. I implemented a change such that they are readable in the output. 


scrape_allrecipes <- function(URL) {
  recipe <- read_html(URL)
  
  title <- html_nodes(recipe, ".headline-wrapper") %>% 
    html_text()
  
  ingredients <- html_nodes(recipe, ".ingredients-item-name") %>% 
    html_text() %>% # This tells `rvest` that the element is text.
    trimws()
  
  directions <- html_nodes(recipe, ".instructions-section-item") %>% 
    html_text() %>%  # This tells `rvest` that the element is text.
    trimws()
  
  ingredients <- gsub("⅛", "1/8", ingredients)
  ingredients <- gsub("⅓", "1/3", ingredients)
  
  directions <- gsub(" Advertisement", "", directions)
  directions <- gsub(" {2,}", " ", directions)
  directions <- str_trim(directions, "right")
  
  print(title)
  print(ingredients)
  print(directions)
  
  write.table(c(title, ingredients, directions), 
              file = "scraped_recipes.txt", 
              append = TRUE, 
              row.names = FALSE, 
              fileEncoding = "UTF-8",
              quote = FALSE)
}

# Insert allrecipe recipe URLs here

recipe_urls <- c(
  "https://www.allrecipes.com/recipe/8386501/french-vanilla-cake-with-french-vanilla-buttercream-frosting/",
  "https://www.allrecipes.com/recipe/19247/soft-oatmeal-cookies/",
  "https://www.allrecipes.com/recipe/8463805/rainbow-sheet-cake/"

)

# Run this for loop to acquire multiple recipes at a time

recipes <- for (recipe_url in recipe_urls) {
  scrape_allrecipes(recipe_url)
}

